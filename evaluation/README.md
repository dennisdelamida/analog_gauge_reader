# Evaluation of Pipeline

Here we have several scripts to evaluate the pipeline on a test dataset.

Follow the following steps for this.

## Create true annotations

Create a file true annotations. For this just run the script `evaluation_file_gen.py`. For the argument `--path` specify the path to the test_images folder. Make sure the images are not directly in this folder but in the subdirectory `test_images/images`.

```shell
python evaluation/evaluation_file_gen.py --path path/to/test_images
```

This will create a skeleton json file where for each image you manually fill out the reading, the unit and the range of the gauge. The range is there to compute the relative error. For this specify the absolute length of the scale. For example if the scale goes from -2 to 5, the range would be 7.

## Run the test images through the pipelines to get results

For this use the command to execute the pipeline:

```shell
python pipeline.py --detection_model path/to/detection_model --segmentation_model /path/to/segmentation_model --key_point_model path/to/key_point_model --base_path path/to/results --debug --input path/to/test_image_folder/images
```

This will create a run folder into the results folder specified. In the run folder you get one folder per image.

## Run evaluation script

Once we have both the true annotations and the results from the pipeline we can compare them with the evaluation script:

```shell
python evaluation/evaluation.py --run_path path/to/run_path --true_readings_path path/to/true_readings_file
```

The `run_path` here is the folder generated by the `pipeline.py` script.

Running this script does two things. For one it summarizes the individual results of the images in the run folder into a single file it saves into the run folder.
Secondly it generates a file called `evaluation.json` in the run folder which summarizes the results.
